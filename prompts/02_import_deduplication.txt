Voc√™ √© um engenheiro backend Go s√™nior respons√°vel EXCLUSIVAMENTE por implementar
IMPORTA√á√ÉO + DEDUPLICA√á√ÉO conforme `AI_DEV_DIRECTIVE.md` e PROMPT 01.

STACK T√âCNICA:
- Backend: Go (Golang) 1.21+ + Gin framework
- Banco: Google Cloud Firestore
- Storage: Google Cloud Storage (GCS)
- Processamento de imagens: biblioteca imaging/draw (Go)

IMPORTANTE:
N√ÉO criar frontend.
N√ÉO criar novas features fora do escopo de importa√ß√£o/deduplica√ß√£o.
N√ÉO alterar regras de governan√ßa (Im√≥vel √önico, canonical listing, co-corretagem, owner passivo).
EXECUTAR ap√≥s PROMPT 01 e PROMPT 09 (auth + multi-tenancy).

========================
OBJETIVO
========================
Importar im√≥veis do CRM (Union) garantindo:
- Property √∫nico (sem duplica√ß√£o)
- Listings m√∫ltiplos por Property, com canonical_listing_id
- Owner (propriet√°rio) criado como entidade PASSIVA, mesmo que dados estejam incompletos
- Auditoria completa (ActivityLog)
- Estrutura pronta para outros layouts no futuro (adapter-based)

========================
ARQUIVOS DE ENTRADA (UNION)
========================
No MVP, o source=union √© composto por DOIS arquivos complementares:

1) XML (principal): /914802.xml
   - cont√©m dados ricos do im√≥vel, descri√ß√£o, SEO, fotos, valores etc.
   - N√ÉO cont√©m todos os dados necess√°rios do propriet√°rio (Owner) ou pode vir incompleto

2) XLS (complementar): arquivo .xls exportado do CRM
   - objetivo: ENRIQUECER campos ausentes do XML, principalmente Owner
   - deve ser tratado como "enrichment dataset" do mesmo source=union

Premissa do MVP:
- A importa√ß√£o deve funcionar mesmo se o XLS estiver ausente (criar Owner placeholder).
- Se o XLS estiver presente, aplicar enrichment sem quebrar o Property √∫nico.

========================
ESCOPO
========================
- Importa√ß√£o via adapters por source
- Implementar APENAS source=union
- Normaliza√ß√£o (XML + XLS ‚Üí payload can√¥nico interno)
- Deduplica√ß√£o de Property
- Cria√ß√£o/atualiza√ß√£o de Owner (passivo, pode ser incompleto)
- Cria√ß√£o de Listing
- Defini√ß√£o de canonical_listing_id
- ActivityLog completo
- Batch com resumo de importa√ß√£o (contagens, duplicados, enriquecidos, erros)

========================
CHAVES DE MATCH (XML ‚Üî XLS)
========================
A importa√ß√£o deve tentar correlacionar o registro do XLS ao im√≥vel do XML.
Ordem de prefer√™ncia (quando dispon√≠veis em ambas as fontes):
1) Codigoimovel (Union) / c√≥digo equivalente no XLS
2) Referencia (ex.: AP00335) / refer√™ncia equivalente no XLS
3) Fallback heur√≠stico (endere√ßo + tipo + √°rea) ‚Äî apenas para sugerir, n√£o para merge autom√°tico

**IMPORTANTE**: A estrutura exata do XLS (nomes de colunas) deve ser analisada
durante a implementa√ß√£o ao examinar o arquivo real. Implementar parser flex√≠vel
que identifica colunas automaticamente (ex: "Codigo", "CodImovel", "CodigoImovel").

Se n√£o for poss√≠vel correlacionar, o im√≥vel ainda √© importado a partir do XML,
e o Owner permanece como placeholder/incompleto (ver regra de Owner).

========================
REGRAS DE DEDUPLICA√á√ÉO (PROPERTY)
========================
1) external_source + external_id (chave forte)
   - external_source = "union"
   - external_id = Codigoimovel (ex.: 77749175)
2) fingerprint heur√≠stico (endere√ßo normalizado + tipo + √°rea total/√∫til)
3) Se d√∫vida ‚Üí possible_duplicate = true (registrar no batch summary)
4) PROIBIDO merge autom√°tico de Properties

========================
OWNER (PROPRIET√ÅRIO) ‚Äî PASSIVO NO MVP
========================
- Todo Property deve apontar para um Owner (owner_id).
- O Owner pode ser criado com dados m√≠nimos se o XML/XLS n√£o tiver dados completos.
- A importa√ß√£o NUNCA deve bloquear por falta de dados do Owner.

Regras:
- Se o XLS fornecer dados do propriet√°rio:
  - criar/atualizar Owner com os campos dispon√≠veis (ex.: nome, telefone, email, documento se existir)
  - registrar consent/origem se houver no XLS
  - ActivityLog: owner_enriched_from_xls
- Se N√ÉO houver dados do propriet√°rio:
  - criar Owner placeholder (ex.: nome="PROPRIET√ÅRIO N√ÉO INFORMADO")
  - marcar flags internas: owner_data_completeness = "missing" | "partial" | "complete"
  - ActivityLog: owner_placeholder_created

IMPORTANTE:
- Owner √© PASSIVO no MVP (sem login, sem leads, sem negocia√ß√£o).
- A confirma√ß√£o passiva por link (quando existir) pode preencher campos depois,
  mas isso N√ÉO faz parte deste prompt.

========================
LISTING E CANONICAL LISTING
========================
- Criar um Listing a partir do XML (conte√∫do de an√∫ncio do operador)
- Primeiro listing vira can√¥nico (canonical_listing_id), a menos que j√° exista um can√¥nico v√°lido
- N√£o trocar automaticamente
- S√≥ trocar se o can√¥nico estiver inativo ou inv√°lido (regra existente)

========================
IMPORTANTE (LEADS)
========================
A importa√ß√£o de im√≥veis N√ÉO cria leads automaticamente.
Leads s√≥ podem ser criados por intera√ß√£o do usu√°rio (ex.: formul√°rio ou WhatsApp),
sempre associados ao Property e nunca diretamente a um corretor.

========================
IMPORTANTE (STATUS/PRE√áO)
========================
A importa√ß√£o N√ÉO define disponibilidade como verdade permanente.
Status e pre√ßo importados devem exigir confirma√ß√£o futura:
- preencher price_amount e status inicial (se dispon√≠vel no XML)
- inicializar status_confirmed_at e price_confirmed_at conforme regra do projeto
  (ex.: setar como "now" apenas se houver evid√™ncia de atualiza√ß√£o recente; caso contr√°rio, marcar como pendente)

========================
MULTI-FONTE / FUTUROS LAYOUTS
========================
- Estrutura por adapters permanece obrigat√≥ria.
- Union √© o √∫nico adapter implementado no MVP.
- O endpoint recebe ?source=union.
- Sem UI de escolha de layout no MVP.
- Deixar o c√≥digo preparado para novos adapters (outros CRMs) sem refatora√ß√£o estrutural.

========================
AUDITORIA (OBRIGAT√ìRIA)
========================
Registrar ActivityLog para:
- import_batch_started / import_batch_completed
- property_created / property_matched_existing
- listing_created
- canonical_listing_assigned
- owner_placeholder_created
- owner_enriched_from_xls
- possible_duplicate_flagged
Cada evento deve conter event_id determin√≠stico, event_hash e request_id.

========================
PROCESSAMENTO DE FOTOS (OBRIGAT√ìRIO)
========================
Ver AI_DEV_DIRECTIVE.md se√ß√£o 14.4 para pipeline completo.

### Pipeline de Importa√ß√£o de Fotos:

1. **Download das URLs Externas (XML)**
   - Ler tags `<Foto>` do XML Union
   - Fazer HTTP GET de cada URL
   - Validar content-type (image/jpeg, image/png, etc.)
   - Tratamento de erro: se download falhar, pular foto + registrar erro

2. **Upload para GCS (Original Tempor√°rio)**
   - Path: `gs://{bucket}/tenants/{tenantId}/properties/{propertyId}/temp/{original_filename}`
   - Manter original APENAS durante processamento
   - Metadados: content-type, source_url, download_timestamp

3. **Processamento e Convers√£o WebP**
   Usar biblioteca: `github.com/disintegration/imaging` (Go)

   Para CADA foto, gerar 3 vers√µes WebP:
   ```go
   // Thumb (listagens)
   thumb := imaging.Resize(img, 400, 300, imaging.Lanczos)
   imaging.Save(thumb, "thumb_400.webp", imaging.JPEGQuality(85))

   // Medium (carrossel)
   medium := imaging.Resize(img, 800, 600, imaging.Lanczos)
   imaging.Save(medium, "medium_800.webp", imaging.JPEGQuality(85))

   // Large (fullscreen)
   large := imaging.Resize(img, 1600, 1200, imaging.Lanczos)
   imaging.Save(large, "large_1600.webp", imaging.JPEGQuality(85))
   ```

4. **Upload Vers√µes Processadas para GCS**
   - Path final: `gs://{bucket}/tenants/{tenantId}/properties/{propertyId}/photos/{photo_id}/`
   - Estrutura:
     ```
     /thumb_400.webp
     /medium_800.webp
     /large_1600.webp
     ```
   - Tornar arquivos **p√∫blicos** (public-read) para CDN
   - Metadados: photo_id, order, is_cover

5. **Excluir Original Tempor√°rio**
   - Ap√≥s sucesso de todas as convers√µes, deletar `/temp/{original_filename}`
   - Economia de storage

6. **Criar Objeto Photo no Listing**
   ```go
   type Photo struct {
       ID       string `firestore:"id"`       // UUID v4
       URL      string `firestore:"url"`      // GCS URL public (large)
       ThumbURL string `firestore:"thumb_url"`  // 400x300
       MediumURL string `firestore:"medium_url"` // 800x600
       LargeURL string `firestore:"large_url"`  // 1600x1200
       Order    int    `firestore:"order"`    // ordem do XML
       IsCover  bool   `firestore:"is_cover"` // primeira foto = true
   }
   ```

7. **Tratamento de Erros**
   - Download falhou: pular foto + registrar em import_errors
   - Convers√£o falhou: pular foto + registrar em import_errors
   - Upload falhou: pular foto + registrar em import_errors
   - NUNCA bloquear importa√ß√£o do Property por erro de foto

8. **ActivityLog**
   - photo_downloaded (sucesso)
   - photo_conversion_failed (erro)
   - photo_uploaded (sucesso)

### Ordem das Fotos:
- Preservar ordem do XML (primeira foto = capa)
- Campo `Order` no Photo: 0, 1, 2, ...
- Campo `IsCover`: true apenas para Order=0

### Performance:
- Processar fotos em **paralelo** (goroutines)
- Limite de 10 fotos simult√¢neas (evitar saturar CPU/rede)
- Timeout de 30s por download de foto

========================
CORRETOR CAPTADOR (OBRIGAT√ìRIO)
========================
Ver AI_DEV_DIRECTIVE.md se√ß√£o 4 e PROMPT 01 (PropertyBrokerRole).

Ao importar um Property, criar PropertyBrokerRole:
```go
PropertyBrokerRole{
    PropertyID: property.ID,
    BrokerID:   authBrokerID, // corretor autenticado que fez importa√ß√£o
    TenantID:   tenantID,
    Role:       "originating_broker", // CAPTADOR
    IsPrimary:  true, // recebe leads por padr√£o
    CommissionPercentage: 0, // opcional no MVP
}
```

**REGRA CR√çTICA**: Todo Property importado DEVE ter 1 originating_broker.
Se importa√ß√£o for manual (UI), usar corretor autenticado.
Se importa√ß√£o for autom√°tica (job), definir corretor default do tenant.

========================
MULTI-TENANCY NA IMPORTA√á√ÉO
========================
Importa√ß√£o SEMPRE associada a um tenant espec√≠fico.

Endpoint:
```
POST /api/v1/tenants/{tenantId}/import
Authorization: Bearer {firebase_token}
Content-Type: multipart/form-data

Body:
- xml_file: arquivo XML
- xls_file: arquivo XLS (opcional)
- source: "union"
```

Valida√ß√µes:
- Middleware valida que token.tenant_id == tenantId da URL
- Apenas admin ou broker do tenant pode importar
- Todos os Properties/Owners/Listings criados pertencem ao tenantID

========================
ESTRUTURA GO (ADICIONAL AO PROMPT 01)
========================

```
internal/
‚îú‚îÄ‚îÄ adapters/
‚îÇ   ‚îî‚îÄ‚îÄ union/
‚îÇ       ‚îú‚îÄ‚îÄ xml_parser.go       # Parse XML Union
‚îÇ       ‚îú‚îÄ‚îÄ xls_parser.go       # Parse XLS Union
‚îÇ       ‚îú‚îÄ‚îÄ normalizer.go       # XML+XLS ‚Üí PropertyPayload
‚îÇ       ‚îî‚îÄ‚îÄ photo_processor.go  # Download + WebP + GCS
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ import_service.go       # Orquestra√ß√£o importa√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ deduplication_service.go # L√≥gica de deduplica√ß√£o
‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îî‚îÄ‚îÄ import_handler.go       # POST /import
```

### Modelos Adicionais:

**ImportBatch**
Cole√ß√£o: `/tenants/{tenantId}/import_batches/{batchId}`
```go
type ImportBatch struct {
    ID        string    `firestore:"-"`
    TenantID  string    `firestore:"tenant_id"`
    Source    string    `firestore:"source"` // "union"
    Status    string    `firestore:"status"` // processing, completed, failed

    // Resumo
    TotalXMLRecords              int `firestore:"total_xml_records"`
    TotalPropertiesCreated       int `firestore:"total_properties_created"`
    TotalPropertiesMatchedExisting int `firestore:"total_properties_matched_existing"`
    TotalPossibleDuplicates      int `firestore:"total_possible_duplicates"`
    TotalOwnersPlaceholders      int `firestore:"total_owners_placeholders"`
    TotalOwnersEnrichedFromXLS   int `firestore:"total_owners_enriched_from_xls"`
    TotalPhotosProcessed         int `firestore:"total_photos_processed"` // NOVO
    TotalErrors                  int `firestore:"total_errors"`

    // Metadados
    StartedAt   time.Time `firestore:"started_at"`
    CompletedAt *time.Time `firestore:"completed_at,omitempty"`
    CreatedBy   string    `firestore:"created_by"` // broker_id
}
```

**ImportError**
Subcole√ß√£o: `/tenants/{tenantId}/import_batches/{batchId}/errors/{errorId}`
```go
type ImportError struct {
    ID          string                 `firestore:"-"`
    BatchID     string                 `firestore:"batch_id"`
    TenantID    string                 `firestore:"tenant_id"`
    ErrorType   string                 `firestore:"error_type"` // xml_parse, photo_download, deduplication, etc.
    ErrorMessage string                `firestore:"error_message"`
    RecordData  map[string]interface{} `firestore:"record_data"` // dados do registro com problema
    Timestamp   time.Time              `firestore:"timestamp"`
}
```

========================
ENTREGA ESPERADA
========================
1. **C√≥digo Modular**:
   - Adapter Union: XML parser + XLS parser + normalizer + photo processor
   - Import Service: orquestra√ß√£o + deduplica√ß√£o
   - Handler: endpoint POST /import

2. **Processamento de Fotos**:
   - Download URLs externas
   - Convers√£o WebP (3 tamanhos)
   - Upload GCS com paths estruturados
   - Cleanup de arquivos tempor√°rios

3. **PropertyBrokerRole**:
   - Cria√ß√£o autom√°tica do originating_broker
   - Associa√ß√£o ao corretor que importou

4. **Multi-tenancy**:
   - Isolamento por tenant
   - Valida√ß√£o de permiss√µes

5. **Batch Summary** (ImportBatch):
   - total_xml_records
   - total_properties_created
   - total_properties_matched_existing
   - total_possible_duplicates
   - total_owners_placeholders
   - total_owners_enriched_from_xls
   - total_photos_processed ‚≠ê NOVO
   - total_errors

6. **Tratamento de Erros**:
   - Subcole√ß√£o import_errors
   - UI privada: exibir erros + bot√£o "Revisar"
   - Resolu√ß√£o manual posterior

7. **Auditoria Completa** (ActivityLog):
   - import_batch_started
   - import_batch_completed
   - property_created
   - property_matched_existing
   - listing_created
   - canonical_listing_assigned
   - owner_placeholder_created
   - owner_enriched_from_xls
   - possible_duplicate_flagged
   - photo_downloaded
   - photo_uploaded
   - photo_conversion_failed

8. **Testes**:
   - Unit tests: parsers XML/XLS
   - Integration tests: import completo com arquivo real (914802.xml)
   - Teste de erro: URLs inv√°lidas, XML malformado

========================
üÜï PROCESSAMENTO DE M√çDIA COM IA (Se√ß√£o 23)
========================

**IMPORTANTE**: Esta se√ß√£o implementa a Se√ß√£o 23 do AI_DEV_DIRECTIVE (Otimiza√ß√£o Autom√°tica de M√≠dia).
A importa√ß√£o deve processar fotos e v√≠deos com an√°lise de qualidade por IA.

### PIPELINE DE FOTOS (Vision API)

**Objetivo**: Baixar fotos do XML, converter para WebP, analisar qualidade e detectar tipo de c√¥modo.

#### Etapa 1: Download e Upload Inicial

```go
// internal/services/photo_processor.go
func (p *PhotoProcessor) ProcessPhotoFromURL(ctx context.Context, photoURL string, propertyID string, order int) (*Photo, error) {
    // 1. Download da foto original
    resp, err := http.Get(photoURL)
    if err != nil {
        return nil, fmt.Errorf("failed to download photo: %w", err)
    }
    defer resp.Body.Close()

    // 2. Decodificar imagem
    img, format, err := image.Decode(resp.Body)
    if err != nil {
        return nil, fmt.Errorf("failed to decode image: %w", err)
    }

    // 3. Gerar 3 tamanhos em WebP
    sizes := map[string]image.Point{
        "thumb":  {400, 300},   // 400x300
        "medium": {800, 600},   // 800x600
        "large":  {1600, 1200}, // 1600x1200
    }

    urls := make(map[string]string)
    for sizeName, dimensions := range sizes {
        // Resize mantendo aspect ratio
        resized := imaging.Fit(img, dimensions.X, dimensions.Y, imaging.Lanczos)

        // Converter para WebP (85% quality)
        buf := new(bytes.Buffer)
        if err := webp.Encode(buf, resized, &webp.Options{Quality: 85}); err != nil {
            return nil, fmt.Errorf("failed to encode webp: %w", err)
        }

        // Upload para GCS
        gcsPath := fmt.Sprintf("tenants/%s/properties/%s/photos/%s_%s.webp",
            p.tenantID, propertyID, photoID, sizeName)

        if err := p.gcsClient.Upload(ctx, gcsPath, buf.Bytes(), "image/webp"); err != nil {
            return nil, fmt.Errorf("failed to upload to GCS: %w", err)
        }

        urls[sizeName+"URL"] = p.gcsClient.PublicURL(gcsPath)
    }

    photo := &Photo{
        ID:        photoID,
        URL:       urls["largeURL"],   // URL principal (1600x1200)
        ThumbURL:  urls["thumbURL"],   // 400x300
        MediumURL: urls["mediumURL"],  // 800x600
        LargeURL:  urls["largeURL"],   // 1600x1200
        Order:     order,
        IsCover:   order == 0, // Primeira foto √© capa
    }

    return photo, nil
}
```

#### Etapa 2: An√°lise com Vision API (Async)

```go
// internal/services/vision_analyzer.go
import (
    vision "cloud.google.com/go/vision/apiv1"
    visionpb "cloud.google.com/go/vision/apiv1/visionpb"
)

func (v *VisionAnalyzer) AnalyzePhoto(ctx context.Context, photoURL string) (*PhotoAnalysis, error) {
    client, err := vision.NewImageAnnotatorClient(ctx)
    if err != nil {
        return nil, err
    }
    defer client.Close()

    image := vision.NewImageFromURI(photoURL)

    // 1. Label Detection (detectar tipo de c√¥modo)
    labels, err := client.DetectLabels(ctx, image, nil, 10)
    if err != nil {
        return nil, fmt.Errorf("label detection failed: %w", err)
    }

    // 2. Image Properties (analisar qualidade)
    props, err := client.DetectImageProperties(ctx, image, nil)
    if err != nil {
        return nil, fmt.Errorf("property detection failed: %w", err)
    }

    // 3. Safe Search (evitar conte√∫do inadequado)
    safeSearch, err := client.DetectSafeSearch(ctx, image, nil)
    if err != nil {
        return nil, fmt.Errorf("safe search failed: %w", err)
    }

    analysis := &PhotoAnalysis{
        Labels:     extractLabels(labels),
        Quality:    calculateQuality(props),
        RoomType:   detectRoomType(labels),
        SafeSearch: safeSearch.Adult != visionpb.Likelihood_LIKELY &&
                     safeSearch.Violence != visionpb.Likelihood_LIKELY,
    }

    return analysis, nil
}

// Detectar tipo de c√¥modo baseado em labels
func detectRoomType(annotations []*visionpb.EntityAnnotation) string {
    labelScores := make(map[string]float32)
    for _, label := range annotations {
        labelScores[strings.ToLower(label.Description)] = label.Score
    }

    // Mapear labels para room_type
    roomKeywords := map[string][]string{
        "living_room": {"living room", "sofa", "couch", "tv", "lounge"},
        "kitchen":     {"kitchen", "stove", "refrigerator", "sink", "oven"},
        "bedroom":     {"bedroom", "bed", "mattress", "pillow", "nightstand"},
        "bathroom":    {"bathroom", "toilet", "shower", "bathtub", "sink"},
        "exterior":    {"building", "facade", "exterior", "garden", "balcony", "terrace"},
    }

    bestMatch := "other"
    bestScore := float32(0.0)

    for roomType, keywords := range roomKeywords {
        score := float32(0.0)
        for _, keyword := range keywords {
            if s, ok := labelScores[keyword]; ok {
                score += s
            }
        }
        if score > bestScore {
            bestScore = score
            bestMatch = roomType
        }
    }

    // Retornar "other" se confian√ßa < 40%
    if bestScore < 0.4 {
        return "other"
    }

    return bestMatch
}

// Calcular qualidade da foto (0.0 - 1.0)
func calculateQuality(props *visionpb.ImageProperties) float64 {
    quality := 1.0

    // 1. Avaliar cores dominantes (diversidade √© bom)
    if len(props.DominantColors.Colors) < 3 {
        quality -= 0.2 // Muito monocrom√°tica
    }

    // 2. Avaliar brilho (0.0 muito escuro, 1.0 muito claro)
    // Ideal: 0.3 - 0.7
    avgBrightness := calculateAverageBrightness(props.DominantColors.Colors)
    if avgBrightness < 0.3 || avgBrightness > 0.7 {
        quality -= 0.3 // Muito escura ou muito clara
    }

    // 3. Crop suggestions (se Vision API sugerir crop, foto n√£o est√° bem enquadrada)
    // Nota: Crop detection requer anota√ß√£o adicional
    // quality -= cropPenalty

    // Normalizar para 0.0 - 1.0
    if quality < 0 {
        quality = 0
    }

    return quality
}

func calculateAverageBrightness(colors []*visionpb.ColorInfo) float64 {
    totalBrightness := 0.0
    for _, c := range colors {
        // F√≥rmula de lumin√¢ncia: 0.299*R + 0.587*G + 0.114*B
        brightness := (0.299*float64(c.Color.Red) +
                      0.587*float64(c.Color.Green) +
                      0.114*float64(c.Color.Blue)) / 255.0
        totalBrightness += brightness * float64(c.PixelFraction)
    }
    return totalBrightness
}
```

#### Etapa 3: Ordena√ß√£o Inteligente

```go
// internal/services/photo_sorter.go
func (s *PhotoSorter) SuggestOrder(photos []*Photo) {
    // Ordem ideal: exterior ‚Üí living_room ‚Üí kitchen ‚Üí bedroom ‚Üí bathroom ‚Üí other

    roomPriority := map[string]int{
        "exterior":    0,
        "living_room": 1,
        "kitchen":     2,
        "bedroom":     3,
        "bathroom":    4,
        "other":       5,
    }

    // Criar grupos por tipo de c√¥modo
    groups := make(map[string][]*Photo)
    for _, photo := range photos {
        roomType := photo.RoomType
        if roomType == "" {
            roomType = "other"
        }
        groups[roomType] = append(groups[roomType], photo)
    }

    // Ordenar cada grupo por qualidade (melhor primeiro)
    for _, group := range groups {
        sort.Slice(group, func(i, j int) bool {
            return group[i].Quality > group[j].Quality
        })
    }

    // Montar ordem final
    suggestedOrder := 0
    for priority := 0; priority <= 5; priority++ {
        for roomType, p := range roomPriority {
            if p == priority {
                if group, ok := groups[roomType]; ok {
                    for _, photo := range group {
                        photo.SuggestedOrder = suggestedOrder
                        suggestedOrder++
                    }
                }
            }
        }
    }
}
```

#### Etapa 4: Trigger Async (Cloud Functions)

**IMPORTANTE**: A an√°lise Vision API √© CARA e LENTA. Deve ser async.

```go
// Importa√ß√£o: Upload foto ‚Üí GCS ‚Üí Trigger Cloud Function ‚Üí Vision API

// 1. Durante importa√ß√£o: apenas fazer upload
func (p *PhotoProcessor) ProcessPhotoFromURL(ctx context.Context, photoURL string) (*Photo, error) {
    // ... upload para GCS (c√≥digo acima)

    // Publicar mensagem no Pub/Sub para processamento async
    topic := p.pubsubClient.Topic("photo-analysis")
    msg := &PhotoAnalysisRequest{
        TenantID:   p.tenantID,
        PropertyID: propertyID,
        PhotoID:    photo.ID,
        PhotoURL:   photo.LargeURL,
    }

    _, err := topic.Publish(ctx, &pubsub.Message{
        Data: mustMarshalJSON(msg),
    }).Get(ctx)

    return photo, err // Retorna ANTES da an√°lise completar
}

// 2. Cloud Function separada processa async
// functions/photo-analyzer/main.go
func AnalyzePhotoFunction(ctx context.Context, m pubsub.Message) error {
    var req PhotoAnalysisRequest
    json.Unmarshal(m.Data, &req)

    // Executar Vision API
    analysis, err := visionAnalyzer.AnalyzePhoto(ctx, req.PhotoURL)
    if err != nil {
        log.Errorf("Vision API failed: %v", err)
        return err
    }

    // Atualizar Firestore com metadata
    photoRef := firestore.Collection("tenants").Doc(req.TenantID).
        Collection("properties").Doc(req.PropertyID).
        Collection("photos").Doc(req.PhotoID)

    _, err = photoRef.Update(ctx, []firestore.Update{
        {Path: "room_type", Value: analysis.RoomType},
        {Path: "quality", Value: analysis.Quality},
        {Path: "suggested_order", Value: analysis.SuggestedOrder},
    })

    return err
}
```

### PIPELINE DE V√çDEOS (ffmpeg)

**Objetivo**: Processar v√≠deos do XML (se houver), extrair thumbnail e detectar dura√ß√£o.

**Nota**: O XML da Union provavelmente N√ÉO ter√° v√≠deos. Esta funcionalidade √© para upload manual via frontend admin (PROMPT 04b j√° implementado). Durante importa√ß√£o, pular v√≠deos.

#### Processamento de V√≠deo (se necess√°rio no futuro)

```go
// internal/services/video_processor.go
import (
    "os/exec"
    "strconv"
    "strings"
)

func (v *VideoProcessor) ProcessVideoUpload(ctx context.Context, videoPath string) (*Video, error) {
    // 1. Detectar dura√ß√£o com ffprobe
    cmd := exec.Command("ffprobe",
        "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        videoPath)

    output, err := cmd.Output()
    if err != nil {
        return nil, fmt.Errorf("ffprobe failed: %w", err)
    }

    durationStr := strings.TrimSpace(string(output))
    duration, err := strconv.ParseFloat(durationStr, 64)
    if err != nil {
        return nil, fmt.Errorf("invalid duration: %w", err)
    }

    // üÜï 2. COMPRESS√ÉO AUTOM√ÅTICA (otimizar storage + streaming)
    compressedPath := "/tmp/video_compressed.mp4"

    cmd = exec.Command("ffmpeg",
        "-i", videoPath,
        "-c:v", "libx264",           // Codec H.264 (compatibilidade universal)
        "-crf", "28",                // Constant Rate Factor: 28 = boa qualidade, ~50% menor
        "-preset", "medium",         // Preset de encoding (medium = balanceado)
        "-c:a", "aac",               // Codec de √°udio AAC
        "-b:a", "128k",              // Bitrate de √°udio 128kbps
        "-movflags", "+faststart",   // Permite streaming progressivo (play antes de download completo)
        "-maxrate", "2M",            // Bitrate m√°ximo 2Mbps (streaming fluido em 4G)
        "-bufsize", "4M",            // Buffer size para rate control
        compressedPath)

    if err := cmd.Run(); err != nil {
        // Fallback: se compress√£o falhar, usar v√≠deo original
        log.Warnf("Video compression failed, using original: %v", err)
        compressedPath = videoPath
    }

    // 3. Extrair thumbnail (frame do meio do v√≠deo)
    thumbnailPath := "/tmp/thumbnail.jpg"
    middleTime := duration / 2

    cmd = exec.Command("ffmpeg",
        "-ss", fmt.Sprintf("%.2f", middleTime), // Seek para o meio
        "-i", compressedPath,                   // ‚ö†Ô∏è Usar v√≠deo comprimido
        "-vframes", "1",                        // Extrair 1 frame
        "-q:v", "2",                            // Qualidade alta
        thumbnailPath)

    if err := cmd.Run(); err != nil {
        return nil, fmt.Errorf("ffmpeg thumbnail failed: %w", err)
    }

    // 4. Upload thumbnail para GCS
    thumbnailData, _ := os.ReadFile(thumbnailPath)
    thumbnailGCSPath := fmt.Sprintf("tenants/%s/videos/%s_thumb.jpg", tenantID, videoID)
    v.gcsClient.Upload(ctx, thumbnailGCSPath, thumbnailData, "image/jpeg")

    // 5. Upload v√≠deo COMPRIMIDO para GCS
    videoData, _ := os.ReadFile(compressedPath)
    videoGCSPath := fmt.Sprintf("tenants/%s/videos/%s.mp4", tenantID, videoID)
    v.gcsClient.Upload(ctx, videoGCSPath, videoData, "video/mp4")

    // 6. Cleanup de arquivos tempor√°rios
    os.Remove(videoPath)
    if compressedPath != videoPath {
        os.Remove(compressedPath)
    }
    os.Remove(thumbnailPath)

    video := &Video{
        ID:           videoID,
        URL:          v.gcsClient.PublicURL(videoGCSPath),
        ThumbnailURL: v.gcsClient.PublicURL(thumbnailGCSPath),
        Duration:     int(duration),
        Source:       "upload",
        CreatedAt:    time.Now(),
    }

    return video, nil
}
```

**IMPORTANTE**: ffmpeg deve estar instalado no ambiente Cloud Run:

```dockerfile
# Dockerfile
FROM golang:1.21-alpine

# Instalar ffmpeg
RUN apk add --no-cache ffmpeg

COPY . /app
WORKDIR /app
RUN go build -o /app/server ./cmd/api

CMD ["/app/server"]
```

#### üÜï Explica√ß√£o dos Par√¢metros de Compress√£o

**Por que CRF 28?**
- CRF (Constant Rate Factor) controla qualidade vs. tamanho
- Escala: 0 (lossless, enorme) at√© 51 (p√©ssima qualidade, min√∫sculo)
- **CRF 28**: sweet spot para v√≠deos de im√≥veis
  - Qualidade: indistingu√≠vel do original para olho humano
  - Redu√ß√£o: ~40-60% do tamanho original
  - Exemplo: 100MB ‚Üí 40-60MB

**Por que maxrate 2Mbps?**
- Garante streaming fluido em conex√£o 4G (5-10Mbps t√≠pico)
- Evita buffering no player
- 1080p Full HD com CRF 28 cabe confortavelmente em 2Mbps

**Por que faststart?**
- Move metadata (moov atom) para o in√≠cio do arquivo
- Player pode come√ßar a reproduzir ANTES do download completo
- Essencial para boa UX em v√≠deos web

**Tempo de processamento estimado**:
- V√≠deo de 1 minuto (100MB) ‚Üí ~15-30 segundos de compress√£o
- Processamento √© **ass√≠ncrono** (n√£o bloqueia upload do corretor)

**Fallback gracioso**:
- Se compress√£o falhar (arquivo corrompido, codec n√£o suportado):
  - Log de warning
  - Usa v√≠deo original sem compress√£o
  - Upload continua normalmente (n√£o quebra fluxo)

### CUSTOS E OTIMIZA√á√ïES

#### Custos Vision API (Google Cloud)

- **Label Detection**: $1.50 / 1000 imagens
- **Image Properties**: $1.00 / 1000 imagens
- **Safe Search**: $1.00 / 1000 imagens
- **Total por foto**: ~$0.0035 (0,35 centavos por foto)

**Exemplo de custo**:
- Importa√ß√£o de 1.000 im√≥veis com m√©dia de 10 fotos cada = 10.000 fotos
- Custo Vision API: 10.000 √ó $0.0035 = **$35**

#### üÜï Custos de Storage de V√≠deos (Google Cloud Storage)

**Sem compress√£o** (v√≠deo original):
- V√≠deo m√©dio: 100MB
- 1.000 im√≥veis √ó 2 v√≠deos/im√≥vel = 2.000 v√≠deos
- Storage total: 200GB
- Custo mensal: 200GB √ó $0.020/GB = **$4/m√™s** (~R$ 20/m√™s)

**Com compress√£o autom√°tica** (CRF 28):
- V√≠deo comprimido: ~50MB (50% redu√ß√£o)
- 1.000 im√≥veis √ó 2 v√≠deos/im√≥vel = 2.000 v√≠deos
- Storage total: 100GB
- Custo mensal: 100GB √ó $0.020/GB = **$2/m√™s** (~R$ 10/m√™s)

**Economia anual**: $24/ano (~R$ 120/ano) para cada 1.000 im√≥veis

**Bandwidth (Egress)**:
- Primeiros 1TB gr√°tis/m√™s
- V√≠deo de 50MB √ó 10 views = 500MB de egress
- 2.000 v√≠deos √ó 500MB = 1TB (ainda gr√°tis)
- Se exceder 1TB: $0.12/GB

**ROI da compress√£o**:
- Economia de storage: 50%
- Economia de bandwidth: 50%
- Melhor UX: streaming mais r√°pido
- **Custo adicional**: Zero (ffmpeg j√° instalado)

#### Otimiza√ß√µes Recomendadas

1. **Batch Vision API requests**:
   - Processar m√∫ltiplas fotos em um √∫nico request
   - Reduz lat√™ncia e custo de rede

2. **Cache de an√°lises**:
   - Se foto j√° foi analisada (mesmo hash SHA256), reusar resultado
   - Evitar re-an√°lise em reimporta√ß√µes

3. **Processamento async obrigat√≥rio**:
   - NUNCA bloquear importa√ß√£o esperando Vision API
   - Usar Pub/Sub + Cloud Functions

4. **Fallback se Vision API falhar**:
   - Foto importada normalmente, apenas sem metadata de qualidade
   - RoomType = "other", Quality = 0.5 (default)

### FLUXO COMPLETO DE IMPORTA√á√ÉO COM M√çDIA

```
1. Upload XML/XLS ‚Üí Backend
2. Parse XML ‚Üí extrair URLs de fotos
3. Para cada foto:
   a. Download da URL externa
   b. Converter para WebP (3 tamanhos)
   c. Upload para GCS
   d. Publicar mensagem Pub/Sub (async)
   e. Criar Photo no Firestore (sem metadata ainda)
4. Cloud Function (async, paralelo):
   a. Receber mensagem Pub/Sub
   b. Executar Vision API
   c. Atualizar Photo no Firestore com room_type, quality, suggested_order
5. Frontend admin (PROMPT 04b):
   a. Exibe fotos com badges de qualidade
   b. Bot√£o "Aplicar Sugest√£o de IA" reordena fotos
```

### ESTRUTURA DE DADOS ATUALIZADA

```go
type Photo struct {
    ID             string    `firestore:"id" json:"id"`
    URL            string    `firestore:"url" json:"url"`
    ThumbURL       string    `firestore:"thumb_url" json:"thumb_url"`
    MediumURL      string    `firestore:"medium_url" json:"medium_url"`
    LargeURL       string    `firestore:"large_url" json:"large_url"`
    Order          int       `firestore:"order" json:"order"`
    IsCover        bool      `firestore:"is_cover" json:"is_cover"`

    // üÜï Vision API metadata (preenchido async)
    RoomType       string    `firestore:"room_type,omitempty" json:"room_type,omitempty"`
    Quality        float64   `firestore:"quality,omitempty" json:"quality,omitempty"`
    SuggestedOrder int       `firestore:"suggested_order,omitempty" json:"suggested_order,omitempty"`

    // Auditoria
    AnalyzedAt     *time.Time `firestore:"analyzed_at,omitempty" json:"analyzed_at,omitempty"`
    AnalysisError  string     `firestore:"analysis_error,omitempty" json:"analysis_error,omitempty"`
}

type ImportBatch struct {
    // ... campos existentes

    // üÜï Estat√≠sticas de m√≠dia
    TotalPhotosProcessed      int `firestore:"total_photos_processed" json:"total_photos_processed"`
    TotalPhotosAnalyzed       int `firestore:"total_photos_analyzed" json:"total_photos_analyzed"`
    TotalPhotoAnalysisErrors  int `firestore:"total_photo_analysis_errors" json:"total_photo_analysis_errors"`
}
```

========================
CRIT√âRIO DE SUCESSO
========================
‚úÖ XML Union parseado corretamente
‚úÖ XLS enriquece dados do Owner (quando dispon√≠vel)
‚úÖ Deduplica√ß√£o via external_source + external_id
‚úÖ Fingerprint heur√≠stico funcional
‚úÖ Owner placeholder criado se dados incompletos
‚úÖ Fotos baixadas e convertidas para WebP (3 tamanhos) ‚≠ê CR√çTICO
‚úÖ Fotos armazenadas em GCS com paths estruturados
‚úÖ Listing criado com Photos corretamente vinculadas
‚úÖ Canonical listing definido (primeiro Listing)
‚úÖ PropertyBrokerRole criado (originating_broker)
‚úÖ Import batch com resumo completo
‚úÖ Erros salvos em import_errors (n√£o bloqueia importa√ß√£o)
‚úÖ ActivityLog registra todos os eventos
‚úÖ Multi-tenancy respeitado (isolamento por tenant)

üÜï ‚úÖ Vision API integrada para an√°lise de qualidade (Se√ß√£o 23)
üÜï ‚úÖ Detec√ß√£o autom√°tica de tipo de c√¥modo (living_room, kitchen, bedroom, etc.)
üÜï ‚úÖ C√°lculo de qualidade da foto (0.0 - 1.0)
üÜï ‚úÖ Ordena√ß√£o inteligente sugerida por IA (exterior ‚Üí sala ‚Üí cozinha ‚Üí quartos)
üÜï ‚úÖ Processamento async via Pub/Sub + Cloud Functions
üÜï ‚úÖ Safe Search implementado (bloqueia conte√∫do inadequado)
üÜï ‚úÖ Fallback gracioso se Vision API falhar
üÜï ‚úÖ Cache de an√°lises (evita re-processamento)
üÜï ‚úÖ ffmpeg integrado para processamento de v√≠deos
üÜï ‚úÖ Extra√ß√£o de thumbnail (frame do meio do v√≠deo)
üÜï ‚úÖ Detec√ß√£o de dura√ß√£o via ffprobe
üÜï ‚úÖ Dockerfile atualizado com ffmpeg
üÜï ‚úÖ Estat√≠sticas de m√≠dia no ImportBatch
